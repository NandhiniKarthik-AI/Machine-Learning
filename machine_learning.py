# -*- coding: utf-8 -*-
"""Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xj9efC0Ea_h0AJE0KwykTmQfKnMFGkzK
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Generate dummy dataset
np.random.seed(42)  # For reproducibility
num_samples = 100  # Number of data points
num_features = 1   # Number of features

# Generate random input features (X)
X = np.random.rand(num_samples, num_features) * 10  # Features between 0 and 10

# Define true weight (w) and bias (b) for the linear relationship
true_w = np.array([[2]])  # True weight
true_b = 5                # True bias

# Generate the target variable (y) with some noise added
noise = np.random.randn(num_samples, 1) * 2  # Adding some Gaussian noise
y = X.dot(true_w) + true_b + noise  # Linear relationship with noise

# Convert numpy arrays to PyTorch tensors
X_train = torch.tensor(X, dtype=torch.float32)
y_train = torch.tensor(y, dtype=torch.float32)

# Step 2: Define Linear Regression model using PyTorch
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        # Define a single fully connected layer (input_features -> output_features)
        self.linear = nn.Linear(num_features, 1)

    def forward(self, x):
        return self.linear(x)

# Instantiate the model
model = LinearRegressionModel()

# Step 3: Define the loss function and optimizer
loss_fn = nn.MSELoss()  # Mean Squared Error Loss
optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent

# Step 4: Train the model
num_epochs = 1000  # Number of epochs to train
loss_values = []

for epoch in range(num_epochs):
    model.train()  # Set model to training mode

    # Forward pass: Compute predicted y by passing X_train to the model
    y_pred = model(X_train)

    # Compute loss
    loss = loss_fn(y_pred, y_train)

    # Backward pass: Compute gradients
    optimizer.zero_grad()  # Zero gradients before backpropagation
    loss.backward()  # Backpropagation

    # Update the model parameters
    optimizer.step()

    # Store the loss value for visualization
    loss_values.append(loss.item())

    # Print the loss every 100 epochs
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Step 5: Visualize the results
# Plot the loss over epochs
plt.figure(figsize=(10, 6))
plt.plot(loss_values, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Over Time')
plt.legend()
plt.show()

# Plot the predictions vs true values
model.eval()  # Set model to evaluation mode
with torch.no_grad():  # No need to track gradients during evaluation
    y_pred = model(X_train)

# Convert tensors back to numpy for plotting
y_pred = y_pred.numpy()

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, label='True Data', color='blue')
plt.plot(X, y_pred, label='Fitted Line', color='red')
plt.xlabel('X')
plt.ylabel('y')
plt.title('True vs Predicted Data')
plt.legend()
plt.show()

# Print the learned parameters
for name, param in model.named_parameters():
    print(f'{name}: {param.data}')